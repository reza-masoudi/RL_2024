{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pwCaZX3mf9O",
        "outputId": "0e1983a5-3d64-4943-a35c-a5061bee8b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t-mwORTUlRLF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple, deque\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoeUO3zWlRLJ"
      },
      "source": [
        "# Definition of a 3-layer neural net with tanh activation\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "19xScikUlRLL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        self.activation_function= nn.Tanh()\n",
        "\n",
        "        self.layer1 = nn.Linear( #<--- linear layer\n",
        "            n_inputs, #<----------------#input features\n",
        "            64,#<-----------------------#output features\n",
        "            bias=bias)#<----------------bias\n",
        "\n",
        "        self.layer2 = nn.Linear(\n",
        "            64,\n",
        "            32,\n",
        "            bias=bias)\n",
        "\n",
        "        self.layer3 = nn.Linear(\n",
        "                    32,\n",
        "                    n_outputs,\n",
        "                    bias=bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation_function( self.layer1(x) )\n",
        "        x = self.activation_function( self.layer2(x) )\n",
        "        y = self.layer3(x)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_eLFU0blRLM"
      },
      "source": [
        "# Q network definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "QZ39zCL-lRLM"
      },
      "outputs": [],
      "source": [
        "class Q_network(nn.Module):\n",
        "    def __init__(self, env, learning_rate=1e-4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define number of inputs (state features) and outputs (actions)\n",
        "        n_inputs = env.observation_space.shape[0]  # CartPole state has 4 features\n",
        "        n_outputs = env.action_space.n  # CartPole has 2 possible actions\n",
        "\n",
        "        # Initialize the neural network\n",
        "        self.network = Net(n_inputs, n_outputs)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
        "\n",
        "    def greedy_action(self, state):\n",
        "        \"\"\"Select the action with the highest Q-value.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            q_values = self.network(state)\n",
        "            greedy_a = torch.argmax(q_values).item()\n",
        "        return greedy_a\n",
        "\n",
        "    def get_qvalues(self, state):\n",
        "      \"\"\"Return Q-values for the given state.\"\"\"\n",
        "      q_values = self.network(state)\n",
        "      return q_values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gharOAZ3lRLN"
      },
      "source": [
        "## Experience replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "99zdRJ3ClRLN"
      },
      "outputs": [],
      "source": [
        "class Experience_replay_buffer:\n",
        "    def __init__(self, memory_size=50000, burn_in=10000):\n",
        "        self.memory_size = memory_size\n",
        "        self.burn_in = burn_in\n",
        "        self.Buffer = namedtuple('Buffer', ['state', 'action', 'reward', 'done', 'next_state'])\n",
        "        self.replay_memory = deque(maxlen=memory_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        \"\"\"Sample a batch of experiences from the replay buffer.\"\"\"\n",
        "        samples = np.random.choice(len(self.replay_memory), batch_size, replace=False)\n",
        "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
        "        return batch\n",
        "\n",
        "    def append(self, s_0, a, r, d, s_1):\n",
        "        \"\"\"Add new experience to the buffer.\"\"\"\n",
        "        self.replay_memory.append(self.Buffer(s_0, a, r, d, s_1))\n",
        "\n",
        "    def burn_in_capacity(self):\n",
        "        \"\"\"Check the fraction of burn-in completed.\"\"\"\n",
        "        return len(self.replay_memory) / self.burn_in\n",
        "\n",
        "    def capacity(self):\n",
        "        \"\"\"Check the fraction of the replay buffer filled.\"\"\"\n",
        "        return len(self.replay_memory) / self.memory_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5TEby1LlRLN"
      },
      "source": [
        "# DDQN agent implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SUXA2lWOlRLO"
      },
      "outputs": [],
      "source": [
        "def from_tuple_to_tensor(tuple_of_np):\n",
        "    tensor = torch.zeros((len(tuple_of_np), tuple_of_np[0].shape[0]))\n",
        "    for i, x in enumerate(tuple_of_np):\n",
        "        tensor[i] = torch.FloatTensor(x)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DDQN_agent:\n",
        "    def __init__(self, env, rew_thre, buffer, learning_rate=0.001, initial_epsilon=0.5, batch_size=64):\n",
        "        self.env = env\n",
        "        self.network = Q_network(env, learning_rate).to(device)\n",
        "        self.target_network = deepcopy(self.network)\n",
        "        self.buffer = buffer\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.reward_threshold = rew_thre\n",
        "        self.window = 50\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "      self.training_rewards = []\n",
        "      self.training_loss = []\n",
        "      self.update_loss = []\n",
        "      self.mean_training_rewards = []\n",
        "      self.sync_eps = []\n",
        "      self.rewards = 0\n",
        "      self.step_count = 0\n",
        "      self.episode = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def take_step(self, mode='exploit'):\n",
        "        # choose action with epsilon greedy\n",
        "        if mode == 'explore':\n",
        "                action = self.env.action_space.sample()\n",
        "        else:\n",
        "                action = self.network.greedy_action(torch.FloatTensor(self.s_0).to(device))\n",
        "\n",
        "        #simulate action\n",
        "        s_1, r, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Store the experience in the replay buffer\n",
        "        self.buffer.append(self.s_0, action, r, done, s_1)\n",
        "\n",
        "        self.rewards += r\n",
        "        self.s_0 = s_1.copy()\n",
        "        self.step_count += 1\n",
        "\n",
        "        if done:\n",
        "          self.s_0, _ = self.env.reset()\n",
        "        return done\n",
        "\n",
        "    # Implement DQN training algorithm\n",
        "    def train(self, gamma=0.99, max_episodes=10000,\n",
        "              network_update_frequency=10,\n",
        "              network_sync_frequency=200):\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.s_0, _ = self.env.reset()\n",
        "\n",
        "        # Populate replay buffer\n",
        "        while self.buffer.burn_in_capacity() < 1:\n",
        "            self.take_step(mode='explore')\n",
        "        ep = 0\n",
        "        training = True\n",
        "        self.populate = False\n",
        "        while training:\n",
        "            self.s_0, _ = self.env.reset()\n",
        "\n",
        "            self.rewards = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                if ((ep % 5) == 0):\n",
        "                    self.env.render()\n",
        "\n",
        "                p = np.random.random()\n",
        "                if p < self.epsilon:\n",
        "                    done = self.take_step(mode='explore')\n",
        "                    # print(\"explore\")\n",
        "                else:\n",
        "                    done = self.take_step(mode='exploit')\n",
        "                    # print(\"train\")\n",
        "                # Update network\n",
        "                if self.step_count % network_update_frequency == 0:\n",
        "                    self.update()\n",
        "                # Sync networks\n",
        "                if self.step_count % network_sync_frequency == 0:\n",
        "                  self.target_network.load_state_dict(self.network.state_dict())\n",
        "                  self.sync_eps.append(self.episode)\n",
        "\n",
        "                if done:\n",
        "                    if self.epsilon >= 0.05:\n",
        "                        self.epsilon = self.epsilon * 0.7\n",
        "                    ep += 1\n",
        "                    if self.rewards > 2000:\n",
        "                        self.training_rewards.append(2000)\n",
        "                    elif self.rewards > 1000:\n",
        "                        self.training_rewards.append(1000)\n",
        "                    elif self.rewards > 500:\n",
        "                        self.training_rewards.append(500)\n",
        "                    else:\n",
        "                        self.training_rewards.append(self.rewards)\n",
        "                    if len(self.update_loss) == 0:\n",
        "                        self.training_loss.append(0)\n",
        "                    else:\n",
        "                        self.training_loss.append(np.mean(self.update_loss))\n",
        "                    self.update_loss = []\n",
        "                    mean_rewards = np.mean(self.training_rewards[-self.window:])\n",
        "                    mean_loss = np.mean(self.training_loss[-self.window:])\n",
        "                    self.mean_training_rewards.append(mean_rewards)\n",
        "                    print(\n",
        "                        \"\\rEpisode {:d} Mean Rewards {:.2f}  Episode reward = {:.2f}   mean loss = {:.2f}\\t\\t\".format(\n",
        "                            ep, mean_rewards, self.rewards, mean_loss), end=\"\")\n",
        "\n",
        "                    if ep >= max_episodes:\n",
        "                        training = False\n",
        "                        print('\\nEpisode limit reached.')\n",
        "                        break\n",
        "                    if mean_rewards >= self.reward_threshold:\n",
        "                        training = False\n",
        "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
        "                            ep))\n",
        "                        break\n",
        "        # save models\n",
        "        self.save_models()\n",
        "        # plot\n",
        "        self.plot_training_rewards()\n",
        "\n",
        "    def save_models(self):\n",
        "        torch.save(self.network, \"Q_net\")\n",
        "\n",
        "    def load_models(self):\n",
        "        self.network = torch.load(\"Q_net\")\n",
        "        self.network.eval()\n",
        "\n",
        "    def plot_training_rewards(self):\n",
        "        plt.plot(self.mean_training_rewards)\n",
        "        plt.title('Mean training rewards')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.xlabel('Episods')\n",
        "        plt.show()\n",
        "        plt.savefig('mean_training_rewards.png')\n",
        "        plt.clf()\n",
        "\n",
        "    def calculate_loss(self, batch):\n",
        "      # Extract info from batch\n",
        "      states, actions, rewards, dones, next_states = list(batch)\n",
        "\n",
        "      # Convert data to torch tensors with requires_grad set to True\n",
        "      rewards = torch.FloatTensor(rewards).reshape(-1, 1).to(device).requires_grad_(False)\n",
        "      actions = torch.LongTensor(actions).reshape(-1, 1).to(device).requires_grad_(False)\n",
        "      dones = torch.FloatTensor(dones).reshape(-1, 1).to(device).requires_grad_(False)\n",
        "      states = from_tuple_to_tensor(states).to(device).requires_grad_(True)\n",
        "      next_states = from_tuple_to_tensor(next_states).to(device).requires_grad_(False)\n",
        "\n",
        "      # Ensure gradients are being tracked for the Q-values\n",
        "      q_values = self.network.get_qvalues(states)  # This needs to track gradients\n",
        "      q_values = q_values.gather(1, actions)\n",
        "\n",
        "      # Calculate the target Q-values using the target network with no_grad()\n",
        "      with torch.no_grad():\n",
        "        next_q_values = self.network.get_qvalues(next_states)\n",
        "        next_actions = torch.argmax(next_q_values, dim=1, keepdim=True)\n",
        "        next_q_targets = self.target_network.get_qvalues(next_states)\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_targets.gather(1, next_actions)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = self.loss_function(q_values, target_q_values)\n",
        "\n",
        "      # Debugging: Check if tensors require gradients\n",
        "      print(f\"q_values.requires_grad: {q_values.requires_grad}\")\n",
        "      print(f\"target_q_values.requires_grad: {target_q_values.requires_grad}\")\n",
        "      print(f\"loss.requires_grad: {loss.requires_grad}\")\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "      self.network.optimizer.zero_grad()  # Reset gradients\n",
        "      batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
        "      loss = self.calculate_loss(batch)   # Calculate loss with gradients enabled\n",
        "      loss.backward()                     # Perform backpropagation\n",
        "      self.network.optimizer.step()       # Update the network weights\n",
        "\n",
        "      self.update_loss.append(loss.item())\n",
        "\n",
        "\n",
        "    def initialize(self):\n",
        "        self.training_rewards = []\n",
        "        self.training_loss = []\n",
        "        self.update_loss = []\n",
        "        self.mean_training_rewards = []\n",
        "        self.sync_eps = []\n",
        "        self.rewards = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "    def evaluate(self, eval_env):\n",
        "        done = False\n",
        "        s, _ = eval_env.reset()\n",
        "        rew = 0\n",
        "        while not done:\n",
        "            action = self.network.greedy_action(torch.FloatTensor(s).to(device))\n",
        "            s, r, terminated, truncated, _ = eval_env.step(action)\n",
        "            done = terminated or truncated\n",
        "            rew += r\n",
        "\n",
        "        print(\"Evaluation cumulative reward: \", rew)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAk-zkkblRLP"
      },
      "source": [
        "# Train and evaluate on cartpole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "QoZXxNoilRLQ",
        "outputId": "1e7f2cb6-07d0-4bc7-a203-0ec65f8abff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 1 Mean Rewards 10.00  Episode reward = 10.00   mean loss = 1.03\t\t\rEpisode 2 Mean Rewards 9.50  Episode reward = 9.00   mean loss = 0.52\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 3 Mean Rewards 9.33  Episode reward = 9.00   mean loss = 0.67\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 4 Mean Rewards 9.50  Episode reward = 10.00   mean loss = 0.72\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 5 Mean Rewards 9.40  Episode reward = 9.00   mean loss = 0.74\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 6 Mean Rewards 9.33  Episode reward = 9.00   mean loss = 0.75\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 7 Mean Rewards 9.43  Episode reward = 10.00   mean loss = 0.74\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 8 Mean Rewards 9.38  Episode reward = 9.00   mean loss = 0.73\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 9 Mean Rewards 9.44  Episode reward = 10.00   mean loss = 0.71\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 10 Mean Rewards 9.50  Episode reward = 10.00   mean loss = 0.69\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 11 Mean Rewards 9.64  Episode reward = 11.00   mean loss = 0.67\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "\rEpisode 12 Mean Rewards 9.58  Episode reward = 9.00   mean loss = 0.65\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 13 Mean Rewards 9.54  Episode reward = 9.00   mean loss = 0.63\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 14 Mean Rewards 9.64  Episode reward = 11.00   mean loss = 0.61\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 15 Mean Rewards 9.67  Episode reward = 10.00   mean loss = 0.59\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 16 Mean Rewards 9.62  Episode reward = 9.00   mean loss = 0.57\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 17 Mean Rewards 9.59  Episode reward = 9.00   mean loss = 0.54\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 18 Mean Rewards 9.61  Episode reward = 10.00   mean loss = 0.53\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 19 Mean Rewards 9.53  Episode reward = 8.00   mean loss = 0.51\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n",
            "Episode 20 Mean Rewards 9.55  Episode reward = 10.00   mean loss = 0.49\t\tq_values.requires_grad: True\n",
            "target_q_values.requires_grad: False\n",
            "loss.requires_grad: True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DDQN_agent' object has no attribute 'episode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f71e742c5edc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperience_replay_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQN_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-b87be64c32fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gamma, max_episodes, network_update_frequency, network_sync_frequency)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnetwork_sync_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_eps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DDQN_agent' object has no attribute 'episode'"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "rew_threshold = 200\n",
        "buffer = Experience_replay_buffer()\n",
        "agent = DDQN_agent(env, rew_threshold, buffer)\n",
        "agent.train()\n",
        "\n",
        "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "agent.evaluate(eval_env)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}